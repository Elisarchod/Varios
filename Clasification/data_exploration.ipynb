{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# COVID-19 Data Exploration\n",
    "\n",
    "This notebook contains the preprocessing part of the assignment, which includes data visualization, assessment for feature engineering (transformations, dimension reduction, interaction between features) and finally feature selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8660aea55877>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfunctions\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcountDistinct\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mapproxCountDistinct\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql.functions import countDistinct, approxCountDistinct\n",
    "\n",
    "import pandas as pd\n",
    "from utils.visualization import cat_plot, plot_counts, plot_hist\n",
    "from utils.functions import *\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "\n",
    "# sns.set_style(\"darkgrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local\") \\\n",
    "    .appName(\"COVID-19\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = data_retrieval(spark).sample(False, 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = df.count()\n",
    "m = len(df.columns)\n",
    "(n , m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking for missing values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg(*[(f.count(f.when(f.isnull(c) | f.isnan(c) | (f.col(c) == -1), c))/f.count(c)).alias(c) for c in df.columns]).toPandas().T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature_12 has almost 50% missing values which represent with -1, we'll omit this feature for now "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperation for numercial and categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['feature_time', 'feature_2', 'feature_3', 'feature_4',  'feature_15', 'label']\n",
    "categorical_cols = ['feature_5', 'feature_6', 'feature_7', 'feature_8', 'feature_9', 'feature_10', 'feature_11', 'feature_13', 'feature_14', 'feature_16', 'feature_17', 'feature_18', 'feature_19', 'feature_20', 'feature_21'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "df.groupby('label').count().withColumn('prc', f.round(f.col('count')/f.sum('count').over(Window.partitionBy()), 4)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.select(*(f.col(c).cast(\"int\").alias(c) for c in numerical_cols), *categorical_cols).toPandas()\n",
    "numerical_cols.remove('label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imbalanced data with only 17% of positive labels, we can balance the data by over/under sampling, change the loss function (tune ) or keep as is but look at recall, precision and auc to make sure that the model's predictions are sufficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Categorical features - There aren't many features in the data set but there are some features with many categories, we can remove these features or merge rare categories, I set the threshold for 10 after visualization, I saw many observations with very small values.\n",
    "One of the methods is to merge categories with similar label distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removed categorical features based on visualization distributions:\n",
    "Â \n",
    "- feature 7 uniform distribution of positive label\n",
    "- feature 8 has a very skewed distribution with - we could transform the feature to include only a single category with a high positive rate\n",
    "- feature 12 had a lot of nulls that should be transformed into \"other\" category\n",
    "- feature 14 uniform distribution of positive label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    print(col)\n",
    "    df['col_merge'] = merge_categories(df[col], 50, encode=False)\n",
    "    d = {'value_counts': df['col_merge'].value_counts(), \n",
    "         'prc': df['col_merge'].value_counts(normalize=True),\n",
    "         'labeled': df.groupby('col_merge')['label'].sum(),\n",
    "         'prc_labeled_out': df.groupby('col_merge')['label'].sum()/df['label'].sum(),\n",
    "         'prc_labeled_in': df.groupby('col_merge')['label'].mean()\n",
    "        }\n",
    "    \n",
    "    try:\n",
    "        col_summary = pd.DataFrame(data=d).reset_index().rename(columns={'index':col}).sort_values('prc_labeled_in')\n",
    "        col_summary = pd.melt(col_summary, id_vars=[col, 'value_counts', 'prc', 'labeled'], value_vars=['prc_labeled_out', 'prc_labeled_in']) \n",
    "    except:\n",
    "        col_summary = pd.DataFrame(data=d).reset_index().rename(columns={'col_merge':col}).sort_values('prc_labeled_in')\n",
    "        col_summary = pd.melt(col_summary, id_vars=[col, 'value_counts', 'prc', 'labeled'], value_vars=['prc_labeled_out', 'prc_labeled_in']) \n",
    "    cat_plot(col_summary, col)\n",
    "#     cat_plot(col_summary, col, ('prc_labeled_out', 'prc labeled out-sample'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Numerical features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[numerical_cols].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numerical_cols:\n",
    "    print(col)\n",
    "    plot_hist(df[df['label']==0][col], df[df['label']==1][col], col)\n",
    "#     plot_hist(np.log(df[df['label']==0][col]), np.log(df[df['label']==1][col]), col)\n",
    "#     plot_hist(np.log1p(df[df['label']==0][col].pct_change()), np.log1p(df[df['label']==1][col].pct_change()), col)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked for correlation between the numerical features and found that feature 2 and 3 have a very high correlation (0.97) with each other, I will remove one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.corr())\n",
    "plt.matshow(df.corr())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
